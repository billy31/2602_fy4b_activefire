#!/usr/bin/env python3
"""
train_landsat.py - ÁÅ´ÁÇπÊ£ÄÊµã‰ºòÂåñÁâà
GitHub: https://github.com/billy31/2602_fy4b_activefire

‰ºòÂåñÈáçÁÇπÔºö
1. ÁÆÄÂåñÊçüÂ§±ÂáΩÊï∞ - Á∫ØDice Loss
2. Êï∞ÊçÆËøáÊª§ - min_fg_pixels=50
3. F1Êó©ÂÅú - 3epochÊó†ÊèêÂçáÂç≥ÂÅú
4. ÂÜªÁªìbackboneËÆ≠ÁªÉÁ≠ñÁï•
5. Â≠¶‰π†Áéáwarmup
"""

import os
import sys
import argparse
import logging
import numpy as np
from datetime import datetime

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR
from torch.cuda.amp import autocast, GradScaler
from torch.utils.tensorboard import SummaryWriter

sys.path.insert(0, '/root/codes/fire0226/MambaVision')
from mambavision import create_model

import rasterio

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DEFAULT_DATA_DIR = '/root/autodl-tmp/training'
DEFAULT_PRETRAIN_DIR = '/root/autodl-tmp/pretrained'
DEFAULT_OUTPUT_DIR = '/root/autodl-tmp/training/output'
DEFAULT_TENSORBOARD_DIR = '/root/tf-logs'


# ============================================================================
# ÁÆÄÂåñÁâàÊçüÂ§±ÂáΩÊï∞ - Á∫ØDice LossÔºàÂØπ‰∏çÂπ≥Ë°°Êï∞ÊçÆÊúÄÈ≤ÅÊ£íÔºâ
# ============================================================================

class DiceLoss(nn.Module):
    """Dice Loss - ‰ΩøÁî®sigmoidÔºåÈÄÇÁî®‰∫éÂçïÈÄöÈÅìËæìÂá∫"""
    def __init__(self, smooth=1.0):
        super().__init__()
        self.smooth = smooth
    
    def forward(self, pred, target):
        # pred: [B, 1, H, W], target: [B, H, W]
        probs = torch.sigmoid(pred).squeeze(1)  # [B, H, W]
        target_fg = (target == 1).float()
        
        intersection = (probs * target_fg).sum()
        union = probs.sum() + target_fg.sum()
        
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        return 1 - dice


class FocalDiceLoss(nn.Module):
    """Focal + Dice ÁªÑÂêà - ÈÄÇÁî®‰∫éÂçïÈÄöÈÅìËæìÂá∫"""
    def __init__(self, dice_weight=1.0, focal_weight=0.5, gamma=2.0):
        super().__init__()
        self.dice_weight = dice_weight
        self.focal_weight = focal_weight
        self.gamma = gamma
        self.dice = DiceLoss()
    
    def forward(self, pred, target):
        # Dice
        dice = self.dice(pred, target)
        
        # Focal (‰ΩøÁî®BCE)
        bce = F.binary_cross_entropy_with_logits(pred.squeeze(1), target.float(), reduction='none')
        probs = torch.sigmoid(pred).squeeze(1)
        pt = probs * target.float() + (1 - probs) * (1 - target.float())
        focal = ((1 - pt) ** self.gamma * bce).mean()
        
        return self.dice_weight * dice + self.focal_weight * focal


# ============================================================================
# Ê®°Âûã
# ============================================================================

class SimpleDecoder(nn.Module):
    """ÁÆÄÂåñËß£Á†ÅÂô® - Êõ¥ÊòìËÆ≠ÁªÉ"""
    def __init__(self, encoder_dim, num_classes):
        super().__init__()
        
        # Ê∏êËøõ‰∏äÈááÊ†∑
        self.dec1 = nn.Sequential(
            nn.Conv2d(encoder_dim, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        self.dec2 = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
        )
        
        self.dec3 = nn.Sequential(
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
        )
        
        self.final = nn.Conv2d(64, num_classes, 1)
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
    
    def forward(self, x, input_shape):
        x = self.dec1(x)
        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
        
        x = self.dec2(x)
        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
        
        x = self.dec3(x)
        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
        
        x = self.final(x)
        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)
        
        return x


class FireDetectionModel(nn.Module):
    def __init__(self, model_name='mamba_vision_S', num_classes=2, 
                 input_channels=3, pretrained=True, pretrained_path=None):
        super().__init__()
        
        # È™®Âπ≤
        self.backbone = create_model(model_name, pretrained=False, num_classes=num_classes)
        
        # ‰øÆÊîπËæìÂÖ•
        if input_channels != 3:
            self._modify_input(input_channels)
        
        # Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉ
        if pretrained and pretrained_path:
            self._load_pretrained(pretrained_path)
        
        # ÁÆÄÂåñËß£Á†ÅÂô®
        dims = {'mamba_vision_T': 640, 'mamba_vision_S': 768, 
                'mamba_vision_B': 1024, 'mamba_vision_L': 1568}
        encoder_dim = dims.get(model_name, 768)
        
        self.decoder = SimpleDecoder(encoder_dim, num_classes)
        self.num_classes = num_classes
    
    def _modify_input(self, input_channels):
        if hasattr(self.backbone, 'patch_embed') and hasattr(self.backbone.patch_embed, 'conv_down'):
            conv = self.backbone.patch_embed.conv_down[0]
            new_conv = nn.Conv2d(input_channels, conv.out_channels, 
                               kernel_size=conv.kernel_size, stride=conv.stride,
                               padding=conv.padding, bias=conv.bias is not None)
            with torch.no_grad():
                n = conv.weight.size(1)
                repeat = (input_channels + n - 1) // n
                w = conv.weight.repeat(1, repeat, 1, 1)
                new_conv.weight.copy_(w[:, :input_channels, :, :])
                if conv.bias is not None:
                    new_conv.bias.copy_(conv.bias)
            self.backbone.patch_embed.conv_down[0] = new_conv
    
    def _load_pretrained(self, path):
        ckpt = torch.load(path, map_location='cpu')
        state = ckpt.get('state_dict', ckpt.get('model', ckpt))
        state = {k: v for k, v in state.items() if not k.startswith('head.')}
        self.backbone.load_state_dict(state, strict=False)
        logger.info(f"Loaded pretrained: {path}")
    
    def forward(self, x):
        B, C, H, W = x.shape
        x = self.backbone.patch_embed(x)
        for level in self.backbone.levels:
            x = level(x)
        x = self.backbone.norm(x)
        x = self.decoder(x, (H, W))
        return x
    
    def freeze_backbone(self):
        """ÂÜªÁªìbackboneÔºåÂè™ËÆ≠ÁªÉdecoder"""
        for param in self.backbone.parameters():
            param.requires_grad = False
        logger.info("Backbone frozen, only training decoder")
    
    def unfreeze_backbone(self):
        """Ëß£ÂÜªbackbone"""
        for param in self.backbone.parameters():
            param.requires_grad = True
        logger.info("Backbone unfrozen")


# ============================================================================
# Êï∞ÊçÆÈõÜ - ÊèêÈ´òmin_fg_pixelsËøáÊª§Âô™Â£∞
# ============================================================================

class FireDataset(Dataset):
    def __init__(self, data_dir, region, bands=[7,6,2], mode='train', 
                 split=0.8, seed=42, min_fg_pixels=50):  # ÊèêÈ´òÂà∞50ËøáÊª§Âô™Â£∞
        self.raw_dir = os.path.join(data_dir, region, 'raw')
        self.label_dir = os.path.join(data_dir, region, 'mask_label')
        self.bands = bands
        self.mode = mode
        
        # Êâ´ÊèèÂπ∂ËøáÊª§
        samples = self._scan_samples()
        self.samples = self._filter_fire(samples, min_fg_pixels)
        
        # ÂàíÂàÜ
        np.random.seed(seed)
        indices = np.random.permutation(len(self.samples))
        split_idx = int(len(indices) * split)
        
        if mode == 'train':
            self.indices = indices[:split_idx]
        else:
            self.indices = indices[split_idx:]
        
        logger.info(f"[{mode}] {len(self.indices)} patches (min_fg={min_fg_pixels})")
    
    def _scan_samples(self):
        samples = []
        for f in os.listdir(self.label_dir):
            if '_voting_' in f and f.endswith('.tif'):
                raw_f = f.replace('_voting_', '_').replace('.tif', '.tif')
                raw_path = os.path.join(self.raw_dir, raw_f)
                label_path = os.path.join(self.label_dir, f)
                if os.path.exists(raw_path):
                    samples.append({'raw': raw_path, 'label': label_path})
        return samples
    
    def _filter_fire(self, samples, min_fg):
        filtered = []
        for s in samples:
            try:
                with rasterio.open(s['label']) as src:
                    label = src.read(1)
                fg = (label == 1).sum()
                if fg >= min_fg:
                    s['fg_count'] = int(fg)
                    s['fg_ratio'] = fg / label.size
                    filtered.append(s)
            except:
                pass
        logger.info(f"Filtered: {len(filtered)}/{len(samples)} have >= {min_fg} fire pixels")
        return filtered
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        s = self.samples[self.indices[idx]]
        
        with rasterio.open(s['raw']) as src:
            bands = self.bands if max(self.bands) <= src.count else list(range(1, src.count+1))
            image = src.read(bands)
        
        with rasterio.open(s['label']) as src:
            label = src.read(1)
        
        # Ê£ÄÊü•Êï∞ÊçÆÊúâÊïàÊÄß
        if np.all(image == 0) or np.all(label == 0):
            logger.warning(f"Zero data detected in sample {idx}")
        
        # ÂΩí‰∏ÄÂåñ
        image = image.astype(np.float32)
        for i in range(image.shape[0]):
            b = image[i]
            if b.max() > b.min():
                image[i] = (b - b.min()) / (b.max() - b.min())
            else:
                image[i] = b / (b.max() + 1e-8) if b.max() > 0 else b
        
        if self.mode == 'train':
            image, label = self._augment(image, label)
        
        return torch.from_numpy(image).float(), torch.from_numpy(label.astype(np.int64))
    
    def _augment(self, img, lbl):
        # ÈöèÊú∫Ê∞¥Âπ≥ÁøªËΩ¨
        if np.random.rand() > 0.5:
            img = np.flip(img, axis=2).copy()
            lbl = np.flip(lbl, axis=1).copy()
        # ÈöèÊú∫ÂûÇÁõ¥ÁøªËΩ¨
        if np.random.rand() > 0.5:
            img = np.flip(img, axis=1).copy()
            lbl = np.flip(lbl, axis=0).copy()
        # ÈöèÊú∫ÊóãËΩ¨90Â∫¶ÔºàÊñ∞Â¢ûÔºâ
        if np.random.rand() > 0.5:
            k = np.random.randint(1, 4)  # ÊóãËΩ¨90, 180, Êàñ 270Â∫¶
            img = np.rot90(img, k, axes=(1, 2)).copy()
            lbl = np.rot90(lbl, k).copy()
        return img, lbl


# ============================================================================
# ËÆ≠ÁªÉ
# ============================================================================

def train_epoch(model, loader, criterion, optimizer, device, scaler, use_amp, max_grad_norm=1.0):
    model.train()
    total_loss = 0
    tp = fp = fn = 0
    
    for i, (images, labels) in enumerate(loader):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        
        if use_amp:
            with autocast():
                outputs = model(images)
                loss = criterion(outputs, labels)
            scaler.scale(loss).backward()
            # Ê¢ØÂ∫¶Ë£ÅÂâ™ - Èò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            # Ê¢ØÂ∫¶Ë£ÅÂâ™ - Èò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()
        
        total_loss += loss.item()
        preds = outputs.argmax(dim=1)
        tp += ((preds == 1) & (labels == 1)).sum().item()
        fp += ((preds == 1) & (labels == 0)).sum().item()
        fn += ((preds == 0) & (labels == 1)).sum().item()
        
        if i % 10 == 0:
            p = tp / (tp + fp + 1e-8) * 100
            r = tp / (tp + fn + 1e-8) * 100
            f1 = 2 * p * r / (p + r + 1e-8)
            logger.info(f'  [{i}/{len(loader)}] Loss: {loss.item():.4f} P:{p:.1f}% R:{r:.1f}% F1:{f1:.1f}%')
    
    avg_loss = total_loss / len(loader)
    precision = tp / (tp + fp + 1e-8) * 100
    recall = tp / (tp + fn + 1e-8) * 100
    f1 = 2 * precision * recall / (precision + recall + 1e-8)
    
    logger.info(f'Train - Loss: {avg_loss:.4f} P:{precision:.2f}% R:{recall:.2f}% F1:{f1:.2f}%')
    return avg_loss, f1


@torch.no_grad()
def validate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    tp = fp = fn = 0
    
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        loss = criterion(outputs, labels)
        total_loss += loss.item()
        preds = outputs.argmax(dim=1)
        tp += ((preds == 1) & (labels == 1)).sum().item()
        fp += ((preds == 1) & (labels == 0)).sum().item()
        fn += ((preds == 0) & (labels == 1)).sum().item()
    
    avg_loss = total_loss / len(loader)
    precision = tp / (tp + fp + 1e-8) * 100
    recall = tp / (tp + fn + 1e-8) * 100
    f1 = 2 * precision * recall / (precision + recall + 1e-8)
    iou = tp / (tp + fp + fn + 1e-8) * 100
    
    logger.info(f'Val - Loss: {avg_loss:.4f} F1:{f1:.2f}% IoU:{iou:.2f}% P:{precision:.2f}% R:{recall:.2f}%')
    return avg_loss, f1, iou, precision, recall


# ============================================================================
# WarmupË∞ÉÂ∫¶Âô®
# ============================================================================

class WarmupCosineScheduler:
    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr, min_lr=1e-7):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
    
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # WarmupÈò∂ÊÆµÁ∫øÊÄßÂ¢ûÂä†
            lr = self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # CosineÈÄÄÁÅ´
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        return lr


# ============================================================================
# ÂèØËßÜÂåñ
# ============================================================================

def visualize_predictions(model, loader, device, num_samples=4, save_dir='./visualizations'):
    """ÂèØËßÜÂåñÈ¢ÑÊµãÁªìÊûú"""
    os.makedirs(save_dir, exist_ok=True)
    model.eval()
    
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    
    count = 0
    with torch.no_grad():
        for images, labels in loader:
            if count >= num_samples:
                break
            
            images = images.to(device)
            outputs = model(images)
            probs = F.softmax(outputs, dim=1)[:, 1, :, :]
            preds = outputs.argmax(dim=1)
            
            # ‰øùÂ≠òÂâçÂá†‰∏™Ê†∑Êú¨
            for i in range(min(2, images.size(0))):
                if count >= num_samples:
                    break
                
                fig, axes = plt.subplots(1, 4, figsize=(16, 4))
                
                # ËæìÂÖ•ÂõæÂÉè (RGBÂêàÊàê)
                img = images[i].cpu().numpy()
                if img.shape[0] >= 3:
                    rgb = np.stack([img[0], img[1], img[2]], axis=-1)
                    axes[0].imshow(rgb)
                else:
                    axes[0].imshow(img[0], cmap='gray')
                axes[0].set_title('Input')
                axes[0].axis('off')
                
                # Ê†áÁ≠æ
                axes[1].imshow(labels[i].cpu().numpy(), cmap='hot')
                axes[1].set_title('Ground Truth')
                axes[1].axis('off')
                
                # È¢ÑÊµãÊ¶ÇÁéá
                axes[2].imshow(probs[i].cpu().numpy(), cmap='hot', vmin=0, vmax=1)
                axes[2].set_title('Pred Probability')
                axes[2].axis('off')
                
                # È¢ÑÊµãÁªìÊûú
                axes[3].imshow(preds[i].cpu().numpy(), cmap='hot')
                axes[3].set_title('Prediction')
                axes[3].axis('off')
                
                plt.tight_layout()
                plt.savefig(f'{save_dir}/sample_{count}.png', dpi=150)
                plt.close()
                
                count += 1
    
    logger.info(f"Saved {count} visualizations to {save_dir}")


# ============================================================================
# GitËá™Âä®Êèê‰∫§ÂäüËÉΩ
# ============================================================================

def git_commit_auto(message):
    """Ëá™Âä®Êèê‰∫§‰ª£Á†ÅÂèòÊõ¥Âà∞Git"""
    try:
        import subprocess
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÂèòÊõ¥
        result = subprocess.run(['git', 'status', '--porcelain'], 
                              capture_output=True, text=True, cwd='/root/codes/fire0226/selfCodes')
        
        if result.stdout.strip():
            # ÊúâÂèòÊõ¥ÔºåÊâßË°åÊèê‰∫§
            subprocess.run(['git', 'add', '-A'], cwd='/root/codes/fire0226/selfCodes', check=True)
            subprocess.run(['git', 'commit', '-m', message], cwd='/root/codes/fire0226/selfCodes', check=True)
            
            # Â∞ùËØïÊé®ÈÄÅ
            push_result = subprocess.run(['git', 'push', 'origin', 'main'], 
                                        capture_output=True, text=True, 
                                        cwd='/root/codes/fire0226/selfCodes')
            if push_result.returncode == 0:
                logger.info(f'‚úÖ Git synced: {message[:50]}...')
            else:
                logger.warning('‚ö†Ô∏è Git commit OK but push failed')
        else:
            logger.info('‚ÑπÔ∏è No code changes to commit')
            
    except Exception as e:
        logger.warning(f'‚ö†Ô∏è Git auto-commit failed: {e}')


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='Fire Detection Training')
    
    # Âü∫Êú¨ÂèÇÊï∞
    parser.add_argument('region', type=str, help='Region name (e.g., Asia1)')
    parser.add_argument('--data-dir', type=str, default=DEFAULT_DATA_DIR)
    parser.add_argument('--output-dir', type=str, default=None)
    parser.add_argument('--tensorboard-dir', type=str, default=DEFAULT_TENSORBOARD_DIR)
    
    # Êï∞ÊçÆÂèÇÊï∞
    parser.add_argument('--bands', type=int, nargs='+', default=[7, 6, 2])
    parser.add_argument('--min-fg-pixels', type=int, default=50, 
                       help='Min fire pixels to filter noise (default: 50)')
    
    # Ê®°ÂûãÂèÇÊï∞
    parser.add_argument('--model', type=str, default='mamba_vision_S')
    parser.add_argument('--pretrained', action='store_true', default=True)
    parser.add_argument('--freeze-backbone-epochs', type=int, default=10,
                       help='Freeze backbone for N epochs (default: 10)')
    
    # ËÆ≠ÁªÉÂèÇÊï∞
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=8,
                       help='Batch size (default: 8)')
    parser.add_argument('--lr', type=float, default=1e-4,
                       help='Learning rate (default: 1e-4)')
    parser.add_argument('--weight-decay', type=float, default=0.01)
    parser.add_argument('--max-grad-norm', type=float, default=1.0,
                       help='Max gradient norm for clipping (default: 1.0)')
    parser.add_argument('--num-workers', type=int, default=4)
    parser.add_argument('--warmup-epochs', type=int, default=5,
                       help='Warmup epochs, 5% of total (default: 5)')
    
    # Êó©ÂÅúÂèÇÊï∞ - Âü∫‰∫éF1
    parser.add_argument('--early-stop-patience', type=int, default=3,
                       help='Early stop patience based on F1 (default: 3)')
    parser.add_argument('--early-stop-min-f1', type=float, default=40.0,
                       help='Minimum F1 to consider successful (default: 40%)')
    
    # ÂÖ∂‰ªñ
    parser.add_argument('--use-amp', action='store_true', default=True)
    parser.add_argument('--tensorboard', action='store_true', default=True)
    parser.add_argument('--visualize', action='store_true', default=False)
    
    args = parser.parse_args()
    
    # ËÆ≠ÁªÉÂâçËá™Âä®Êèê‰∫§
    git_commit_auto(f"Pre-train: Start training {args.region} with lr={args.lr}, bs={args.batch_size}")
    
    torch.manual_seed(42)
    np.random.seed(42)
    
    if args.output_dir is None:
        args.output_dir = os.path.join(DEFAULT_OUTPUT_DIR, args.region)
    os.makedirs(args.output_dir, exist_ok=True)
    
    # TensorBoard
    writer = None
    if args.tensorboard:
        exp_name = f"fire_{args.region}_{datetime.now().strftime('%m%d_%H%M')}"
        tb_dir = os.path.join(args.tensorboard_dir, exp_name)
        writer = SummaryWriter(tb_dir)
        logger.info(f'TensorBoard: {tb_dir}')
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f'Device: {device}')
    
    # Êï∞ÊçÆÈõÜ
    train_ds = FireDataset(args.data_dir, args.region, args.bands, 'train', 
                          min_fg_pixels=args.min_fg_pixels)
    val_ds = FireDataset(args.data_dir, args.region, args.bands, 'val',
                        min_fg_pixels=args.min_fg_pixels)
    
    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,
                             num_workers=args.num_workers, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False,
                           num_workers=args.num_workers, pin_memory=True)
    
    # Ê®°Âûã
    pretrained_path = os.path.join(DEFAULT_PRETRAIN_DIR, 'mambavision_small_1k.pth') if args.pretrained else None
    model = FireDetectionModel(args.model, 1, len(args.bands), args.pretrained, pretrained_path).to(device)
    logger.info(f'Params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M')
    
    # ÊçüÂ§± - ÁÆÄÂåñÁâàDice
    criterion = DiceLoss()
    logger.info('Using Dice Loss')
    
    # ‰ºòÂåñÂô®
    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    # Warmup + CosineË∞ÉÂ∫¶
    scheduler = WarmupCosineScheduler(optimizer, args.warmup_epochs, args.epochs, args.lr)
    scaler = GradScaler() if args.use_amp else None
    
    # ËÆ≠ÁªÉÁä∂ÊÄÅ
    best_f1 = 0.0
    best_epoch = 0
    epochs_no_improve = 0
    
    for epoch in range(1, args.epochs + 1):
        current_lr = scheduler.step(epoch - 1)
        logger.info(f'\nEpoch {epoch}/{args.epochs} (lr={current_lr:.2e})')
        logger.info('-' * 60)
        
        # Èò∂ÊÆµÊÄßËß£ÂÜªbackbone - Êõ¥ÁÅµÊ¥ªÁöÑÁ≠ñÁï•
        if epoch == 1 and args.freeze_backbone_epochs > 0:
            model.freeze_backbone()
        elif epoch == args.freeze_backbone_epochs + 1:
            model.unfreeze_backbone()
            # Ëß£ÂÜªÂêé‰ΩøÁî®ËæÉÂ∞èÁöÑÂ≠¶‰π†Áéá
            optimizer = AdamW([
                {'params': model.backbone.parameters(), 'lr': args.lr * 0.1},
                {'params': model.decoder.parameters(), 'lr': args.lr}
            ], weight_decay=args.weight_decay)
            logger.info('Optimizer reinitialized with layer-wise lr')
        
        # ËÆ≠ÁªÉ
        train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device, scaler, args.use_amp, args.max_grad_norm)
        
        # È™åËØÅ
        val_loss, val_f1, val_iou, val_p, val_r = validate(model, val_loader, criterion, device)
        
        # TensorBoard
        if writer:
            writer.add_scalar('Loss/train', train_loss, epoch)
            writer.add_scalar('Loss/val', val_loss, epoch)
            writer.add_scalar('Metrics/F1', val_f1, epoch)
            writer.add_scalar('Metrics/IoU', val_iou, epoch)
            writer.add_scalar('Metrics/Precision', val_p, epoch)
            writer.add_scalar('Metrics/Recall', val_r, epoch)
            writer.add_scalar('Train/lr', current_lr, epoch)
        
        # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°ÂûãÔºàÂü∫‰∫éF1Ôºâ
        if val_f1 > best_f1:
            best_f1 = val_f1
            best_epoch = epoch
            epochs_no_improve = 0
            torch.save({
                'epoch': epoch, 'model': model.state_dict(),
                'f1': val_f1, 'iou': val_iou, 'p': val_p, 'r': val_r,
                'args': vars(args)
            }, os.path.join(args.output_dir, 'best_model.pth'))
            logger.info(f'‚úì Saved best model (F1: {best_f1:.2f}%)')
        else:
            epochs_no_improve += 1
            logger.info(f'  No F1 improvement for {epochs_no_improve} epochs')
        
        # Êó©ÂÅú - 3epochÊó†F1ÊèêÂçáÂç≥ÂÅú
        if epochs_no_improve >= args.early_stop_patience:
            logger.warning(f'\nüõë Early stopping! No F1 improvement for {args.early_stop_patience} epochs')
            logger.warning(f'   Best F1: {best_f1:.2f}% at epoch {best_epoch}')
            
            if best_f1 < args.early_stop_min_f1:
                logger.warning(f'   Warning: Best F1 {best_f1:.2f}% < {args.early_stop_min_f1}% target')
            break
    
    # ÂèØËßÜÂåñ
    if args.visualize:
        visualize_predictions(model, val_loader, device)
    
    logger.info(f'\nüèÜ Best: F1 {best_f1:.2f}% (P:{val_p:.1f}%, R:{val_r:.1f}%) @ epoch {best_epoch}')
    
    # ËÆ≠ÁªÉÂêéËá™Âä®Êèê‰∫§
    git_commit_auto(f"Post-train: {args.region} best F1={best_f1:.2f}% @ epoch {best_epoch}")
    
    if writer:
        writer.close()


if __name__ == '__main__':
    main()
